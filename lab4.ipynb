{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec4a4ab3",
   "metadata": {},
   "source": [
    "# Lab 4\n",
    "# Extend ETL pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d8e3be",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32f8509a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f15f454",
   "metadata": {},
   "source": [
    "#### Adding more rows to the DataFrame\n",
    "- this is the code to add more rows to the DataFrame\n",
    "- otherwise there would be nothing in incremental parts of the assignement\n",
    "- This script generates new records for a custom dataset and appends them to an existing CSV file.\n",
    "- If the file does not exist, it creates a new one with the specified number of records.\n",
    "- The dataset includes transaction details such as date, customer name, product category, amount, quantity, payment method, and status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25a76cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to add 30 new records to 'custom_data.csv'...\n",
      "Successfully added 30 new records to 'custom_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Defined Variables\n",
    "OUTPUT_FILENAME = 'custom_data.csv'\n",
    "NUM_NEW_ROWS = 30\n",
    "SALES_PER_DAY_RANGE = (3, 8) \n",
    "\n",
    "CUSTOMERS = ['Amazon', 'Walmart', 'Target', 'Costco', 'BestBuy', 'eBay', 'Microsoft', 'Google', 'Apple', 'Meta']\n",
    "PRODUCT_CATEGORIES = ['Electronics', 'Home Goods', 'Apparel', 'Books', 'Groceries', 'Software', 'Tools', 'Sports']\n",
    "PAYMENT_METHODS = ['Credit Card', 'Debit Card', 'PayPal', 'Bank Transfer']\n",
    "STATUS_OPTIONS = ['Completed', 'Pending', 'Cancelled', 'Refunded'] \n",
    "\n",
    "# Print statement to indicate 30 rows being added\n",
    "print(f\"Attempting to add {NUM_NEW_ROWS} new records to '{OUTPUT_FILENAME}'...\")\n",
    "\n",
    "# Using Try-Except to handle file reading and record ID management\n",
    "# Using it to avoid using os operations directly\n",
    "try:\n",
    "    existing_df = pd.read_csv(OUTPUT_FILENAME)\n",
    "    if not existing_df.empty:\n",
    "        record_id_counter = existing_df['record_id'].max() + 1\n",
    "    else:\n",
    "        record_id_counter = 1\n",
    "except FileNotFoundError:\n",
    "    print(f\"'{OUTPUT_FILENAME}' not found. Creating a new file with new records.\")\n",
    "    existing_df = pd.DataFrame()\n",
    "    record_id_counter = 1 \n",
    "\n",
    "# Start generating new records from June 1, 2025\n",
    "START_DATE_FOR_NEW_DATA = datetime(2025, 6, 1)\n",
    "\n",
    "# Generate new records, started by creating a new list to hold the new records\n",
    "new_records = []\n",
    "num_records_generated = 0\n",
    "current_date_for_new_data = START_DATE_FOR_NEW_DATA\n",
    "\n",
    "# Using timedelta to generate records for each day\n",
    "while num_records_generated < NUM_NEW_ROWS:\n",
    "  \n",
    "    if num_records_generated > 0: \n",
    "        current_date_for_new_data += timedelta(days=1)\n",
    "\n",
    "    transaction_time = current_date_for_new_data + timedelta(\n",
    "        hours=random.randint(0, 23),\n",
    "        minutes=random.randint(0, 59),\n",
    "        seconds=random.randint(0, 59)\n",
    "    )\n",
    "\n",
    "    last_updated_time = transaction_time + timedelta(\n",
    "        minutes=random.randint(1, 120) \n",
    "    )\n",
    "  \n",
    "    if last_updated_time.date() > current_date_for_new_data.date():\n",
    "        last_updated_time = current_date_for_new_data.replace(hour=23, minute=59, second=59)\n",
    "\n",
    "    new_records.append({\n",
    "        'record_id': record_id_counter,\n",
    "        'transaction_date': current_date_for_new_data.date().isoformat(),\n",
    "        'transaction_timestamp': transaction_time.isoformat(),\n",
    "        'customer_name': random.choice(CUSTOMERS),\n",
    "        'product_category': random.choice(PRODUCT_CATEGORIES),\n",
    "        'amount': round(random.uniform(10.00, 2000.00), 2),\n",
    "        'quantity': random.randint(1, 10),\n",
    "        'payment_method': random.choice(PAYMENT_METHODS),\n",
    "        'last_updated_timestamp': last_updated_time.isoformat(),\n",
    "        'status': random.choice(STATUS_OPTIONS)\n",
    "    })\n",
    "    record_id_counter += 1\n",
    "    num_records_generated += 1\n",
    "\n",
    "# Convert the list of new records to a DataFrame and combine with existing data\n",
    "new_df = pd.DataFrame(new_records)\n",
    "combined_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "combined_df.to_csv(OUTPUT_FILENAME, index=False)\n",
    "\n",
    "print(f\"Successfully added {NUM_NEW_ROWS} new records to '{OUTPUT_FILENAME}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f589143a",
   "metadata": {},
   "source": [
    "#### Loading Original data\n",
    "- setting the path to all the csv files and the extraction text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a11cd008",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = 'custom_data.csv' \n",
    "LAST_EXTRACTION_FILE = 'last_extraction.txt'\n",
    "OUTPUT_FULL_TRANSFORMED_FILENAME = 'transformed_full.csv'\n",
    "OUTPUT_INCREMENTAL_TRANSFORMED_FILENAME = 'transformed_incremental.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d3626f",
   "metadata": {},
   "source": [
    "#### Full extraction\n",
    "- Simply loading the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2eee310c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   record_id transaction_date transaction_timestamp customer_name  \\\n",
      "0          1       2025-04-01   2025-04-01T17:49:57        Costco   \n",
      "1          2       2025-04-01   2025-04-01T10:36:18         Apple   \n",
      "2          3       2025-04-01   2025-04-01T17:16:47        Target   \n",
      "3          4       2025-04-01   2025-04-01T18:15:48        Costco   \n",
      "4          5       2025-04-01   2025-04-01T02:37:35        Amazon   \n",
      "\n",
      "  product_category   amount  quantity payment_method last_updated_timestamp  \\\n",
      "0      Electronics   929.94         1         PayPal    2025-04-01T18:49:57   \n",
      "1        Groceries   130.17         4         PayPal    2025-04-01T12:59:18   \n",
      "2       Home Goods  1333.76         8    Credit Card    2025-04-01T18:19:47   \n",
      "3            Tools  1115.54         9         PayPal    2025-04-01T20:41:48   \n",
      "4         Software   633.14         6    Credit Card    2025-04-01T03:35:35   \n",
      "\n",
      "      status  \n",
      "0    Pending  \n",
      "1  Cancelled  \n",
      "2    Pending  \n",
      "3  Cancelled  \n",
      "4  Cancelled  \n",
      "Shape: (488, 10)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "full_data_df = pd.read_csv(DATA_FILE)\n",
    "print(full_data_df.head())\n",
    "# Shape of the DataFrame\n",
    "print(\"Shape:\", full_data_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec8dea7",
   "metadata": {},
   "source": [
    "### Incremental extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1374a487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last extraction timestamp: 2025-06-17 12:29:13.058419\n",
      "Extracted 68 incremental rows.\n",
      "First 5 rows of incremental data:\n",
      "      record_id transaction_date transaction_timestamp customer_name  \\\n",
      "355        356       2025-06-18   2025-06-18T13:03:27        Amazon   \n",
      "356        357       2025-06-19   2025-06-19T15:37:21        Google   \n",
      "357        358       2025-06-20   2025-06-20T03:05:25        Target   \n",
      "358        359       2025-06-21   2025-06-21T19:45:24        Amazon   \n",
      "359        360       2025-06-22   2025-06-22T17:12:33        Amazon   \n",
      "\n",
      "    product_category   amount  quantity payment_method last_updated_timestamp  \\\n",
      "355            Books  1435.31         5    Credit Card    2025-06-18 13:36:27   \n",
      "356       Home Goods    39.00         3  Bank Transfer    2025-06-19 16:16:21   \n",
      "357      Electronics  1987.26         3         PayPal    2025-06-20 04:32:25   \n",
      "358        Groceries  1267.45         2  Bank Transfer    2025-06-21 20:57:24   \n",
      "359           Sports   470.82         4    Credit Card    2025-06-22 17:40:33   \n",
      "\n",
      "        status  \n",
      "355  Completed  \n",
      "356    Pending  \n",
      "357  Cancelled  \n",
      "358  Cancelled  \n",
      "359  Completed  \n"
     ]
    }
   ],
   "source": [
    "# retrieves the last extraction timestamp from a file\n",
    "def get_last_timestamp(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            timestamp_str = f.read().strip()\n",
    "            if timestamp_str:\n",
    "                return datetime.fromisoformat(timestamp_str)\n",
    "    return datetime.min \n",
    "# get the last extraction timestamp\n",
    "last_extraction_time = get_last_timestamp(LAST_EXTRACTION_FILE)\n",
    "print(f\"Last extraction timestamp: {last_extraction_time}\")\n",
    "\n",
    "incremental_data_df = pd.DataFrame() \n",
    "# Attempting to extract incremental data based on the last extraction timestamp\n",
    "try:\n",
    "    if not full_data_df.empty: \n",
    "        full_data_df['last_updated_timestamp'] = pd.to_datetime(full_data_df['last_updated_timestamp'], errors='coerce')\n",
    "        \n",
    "        incremental_data_df = full_data_df[\n",
    "            full_data_df['last_updated_timestamp'] > last_extraction_time\n",
    "        ].copy() \n",
    "\n",
    "        print(f\"Extracted {incremental_data_df.shape[0]} incremental rows.\")\n",
    "        if not incremental_data_df.empty:\n",
    "            print(\"First 5 rows of incremntal data:\\n\", incremental_data_df.head())\n",
    "        else:\n",
    "            print(\"No new or updated records found.\")\n",
    "    else:\n",
    "        print(\"No full dta to perform incremental extraction from.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during incremental extraction: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465ca651",
   "metadata": {},
   "source": [
    "## Transformations\n",
    "Decided on Using a Fucntion to Transform the Data as it would save on code duplication for the full data and incremental data\n",
    "Performed the following transformations:\n",
    "- removing duplicateds\n",
    "- sttripping whitespace\n",
    "- standardizing\n",
    "- drpooing rows wirh missing values\n",
    "- Data typoe conversions\n",
    "    - string to datetime\n",
    "    - string to numeric/ coercing to numeric to prevent errors\n",
    "- feature engineering \n",
    "- renaming columns for standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cf4946b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_transformations(df):\n",
    "    transformed_df = df.copy()\n",
    "\n",
    "    # Removing duplicates\n",
    "    transformed_df.drop_duplicates(subset=['record_id'], keep='first', inplace=True)\n",
    "\n",
    "    # Stripping whitespaec from string columns\n",
    "    for col in ['customer_name', 'product_category', 'payment_method', 'status']:\n",
    "        if col in transformed_df.columns and transformed_df[col].dtype == 'object':\n",
    "            transformed_df[col] = transformed_df[col].str.strip()\n",
    "\n",
    "    # Standrdizing 'status' casing\n",
    "    if 'status' in transformed_df.columns:\n",
    "        transformed_df['status'] = transformed_df['status'].str.title()\n",
    "\n",
    "    # Handling missing values in critical columns\n",
    "    transformed_df.dropna(subset=['amount', 'quantity', 'transaction_date', 'transaction_timestamp'], inplace=True)\n",
    "\n",
    "    # Data Type Conversion\n",
    "    transformed_df['transaction_date'] = pd.to_datetime(transformed_df['transaction_date'], errors='coerce')\n",
    "    transformed_df['transaction_timestamp'] = pd.to_datetime(transformed_df['transaction_timestamp'], errors='coerce')\n",
    "    transformed_df['last_updated_timestamp'] = pd.to_datetime(transformed_df['last_updated_timestamp'], errors='coerce')\n",
    "    transformed_df['amount'] = pd.to_numeric(transformed_df['amount'], errors='coerce')\n",
    "    transformed_df['quantity'] = pd.to_numeric(transformed_df['quantity'], errors='coerce')\n",
    "\n",
    "    # Feature engineering\n",
    "    transformed_df['total_price'] = transformed_df['quantity'] * transformed_df['amount']\n",
    "\n",
    "    # Column Selection and Renaming\n",
    "    selected_cols = [\n",
    "        'record_id',\n",
    "        'transaction_date',\n",
    "        'transaction_timestamp',\n",
    "        'customer_name',\n",
    "        'product_category',\n",
    "        'quantity',\n",
    "        'amount',\n",
    "        'total_price',\n",
    "        'payment_method',\n",
    "        'status'\n",
    "    ]\n",
    "    transformed_df = transformed_df[[col for col in selected_cols if col in transformed_df.columns]]\n",
    "\n",
    "    transformed_df.rename(columns={\n",
    "        'record_id': 'transaction_id',\n",
    "        'customer_name': 'customer',\n",
    "        'product_category': 'category',\n",
    "        'payment_method': 'payment_type'\n",
    "    }, inplace=True)\n",
    "\n",
    "    return transformed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3949fd9f",
   "metadata": {},
   "source": [
    "#### Transform full data\n",
    "- apply the transfoirmation function to the full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9594a2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed 488 records for full data.\n",
      "First 5 rows of transformed full data:\n",
      "    transaction_id transaction_date transaction_timestamp customer  \\\n",
      "0               1       2025-04-01   2025-04-01 17:49:57   Costco   \n",
      "1               2       2025-04-01   2025-04-01 10:36:18    Apple   \n",
      "2               3       2025-04-01   2025-04-01 17:16:47   Target   \n",
      "3               4       2025-04-01   2025-04-01 18:15:48   Costco   \n",
      "4               5       2025-04-01   2025-04-01 02:37:35   Amazon   \n",
      "\n",
      "      category  quantity   amount  total_price payment_type     status  \n",
      "0  Electronics         1   929.94       929.94       PayPal    Pending  \n",
      "1    Groceries         4   130.17       520.68       PayPal  Cancelled  \n",
      "2   Home Goods         8  1333.76     10670.08  Credit Card    Pending  \n",
      "3        Tools         9  1115.54     10039.86       PayPal  Cancelled  \n",
      "4     Software         6   633.14      3798.84  Credit Card  Cancelled  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 488 entries, 0 to 487\n",
      "Data columns (total 10 columns):\n",
      " #   Column                 Non-Null Count  Dtype         \n",
      "---  ------                 --------------  -----         \n",
      " 0   transaction_id         488 non-null    int64         \n",
      " 1   transaction_date       488 non-null    datetime64[ns]\n",
      " 2   transaction_timestamp  488 non-null    datetime64[ns]\n",
      " 3   customer               488 non-null    object        \n",
      " 4   category               488 non-null    object        \n",
      " 5   quantity               488 non-null    int64         \n",
      " 6   amount                 488 non-null    float64       \n",
      " 7   total_price            488 non-null    float64       \n",
      " 8   payment_type           488 non-null    object        \n",
      " 9   status                 488 non-null    object        \n",
      "dtypes: datetime64[ns](2), float64(2), int64(2), object(4)\n",
      "memory usage: 41.9+ KB\n",
      "\n",
      "Data Types:\n",
      " None\n",
      "Transformed full data saved to 'transformed_full.csv'\n"
     ]
    }
   ],
   "source": [
    "# Check if the DataFrame 'full_data_df' is not empty\n",
    "if not full_data_df.empty:\n",
    "    # If there is data, apply transformation functions to the DataFrame\n",
    "    transformed_full_df = apply_transformations(full_data_df)\n",
    "    \n",
    "    # Print information about the transformation\n",
    "    print(f\"Transformed {len(transformed_full_df)} records for full data.\")\n",
    "    print(\"First 5 rows of transformed full data:\\n\", transformed_full_df.head())\n",
    "    print(\"\\nData Types:\\n\", transformed_full_df.info())  # Shows column data types and memory usage\n",
    "\n",
    "    try:\n",
    "        # Attempt to save the transformed DataFrame to a CSV file\n",
    "        transformed_full_df.to_csv(OUTPUT_FULL_TRANSFORMED_FILENAME, index=False)\n",
    "        print(f\"Transformed full data saved to '{OUTPUT_FULL_TRANSFORMED_FILENAME}'\")\n",
    "    except Exception as e:\n",
    "        # Handle any errors that occur during the save operation\n",
    "        print(f\"Error saving transformed full data: {e}\")\n",
    "else:\n",
    "    # If the input DataFrame was empty, skip the transformation process\n",
    "    print(\"Full data is empty, skipping transformation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f208d344",
   "metadata": {},
   "source": [
    "#### Incremental Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49ccb78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed 68 records for incremental data.\n",
      "First 5 rows of transformed incremental data:\n",
      "      transaction_id transaction_date transaction_timestamp customer  \\\n",
      "355             356       2025-06-18   2025-06-18 13:03:27   Amazon   \n",
      "356             357       2025-06-19   2025-06-19 15:37:21   Google   \n",
      "357             358       2025-06-20   2025-06-20 03:05:25   Target   \n",
      "358             359       2025-06-21   2025-06-21 19:45:24   Amazon   \n",
      "359             360       2025-06-22   2025-06-22 17:12:33   Amazon   \n",
      "\n",
      "        category  quantity   amount  total_price   payment_type     status  \n",
      "355        Books         5  1435.31      7176.55    Credit Card  Completed  \n",
      "356   Home Goods         3    39.00       117.00  Bank Transfer    Pending  \n",
      "357  Electronics         3  1987.26      5961.78         PayPal  Cancelled  \n",
      "358    Groceries         2  1267.45      2534.90  Bank Transfer  Cancelled  \n",
      "359       Sports         4   470.82      1883.28    Credit Card  Completed  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 68 entries, 355 to 487\n",
      "Data columns (total 10 columns):\n",
      " #   Column                 Non-Null Count  Dtype         \n",
      "---  ------                 --------------  -----         \n",
      " 0   transaction_id         68 non-null     int64         \n",
      " 1   transaction_date       68 non-null     datetime64[ns]\n",
      " 2   transaction_timestamp  68 non-null     datetime64[ns]\n",
      " 3   customer               68 non-null     object        \n",
      " 4   category               68 non-null     object        \n",
      " 5   quantity               68 non-null     int64         \n",
      " 6   amount                 68 non-null     float64       \n",
      " 7   total_price            68 non-null     float64       \n",
      " 8   payment_type           68 non-null     object        \n",
      " 9   status                 68 non-null     object        \n",
      "dtypes: datetime64[ns](2), float64(2), int64(2), object(4)\n",
      "memory usage: 5.8+ KB\n",
      "\n",
      "Data Types:\n",
      " None\n",
      "Transformed incremental data saved to 'transformed_incremental.csv'\n"
     ]
    }
   ],
   "source": [
    "# Check if the incremental data DataFrame is not empty\n",
    "if not incremental_data_df.empty:\n",
    "    # Apply the same transformation functions to the incremental data\n",
    "    transformed_incremental_df = apply_transformations(incremental_data_df)\n",
    "    \n",
    "    # Print transformation details for incremental data\n",
    "    print(f\"Transformed {len(transformed_incremental_df)} records for incremental data.\")\n",
    "    print(\"First 5 rows of transformed incremental data:\\n\", transformed_incremental_df.head())\n",
    "    print(\"\\nData Types:\\n\", transformed_incremental_df.info())  # Shows column structure and memory usage\n",
    "\n",
    "    try:\n",
    "        # Attempt to save the transformed incremental data to a different output file\n",
    "        transformed_incremental_df.to_csv(OUTPUT_INCREMENTAL_TRANSFORMED_FILENAME, index=False)\n",
    "        print(f\"Transformed incremental data saved to '{OUTPUT_INCREMENTAL_TRANSFORMED_FILENAME}'\")\n",
    "    except Exception as e:\n",
    "        # Handle any file saving errors specifically for incremental data\n",
    "        print(f\"Error saving transformed incremental data: {e}\")\n",
    "else:\n",
    "    # If no incremental data was available, skip processing\n",
    "    print(\"Incremental data is empty, skipping transformation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b118b812",
   "metadata": {},
   "source": [
    "### Update Last Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8282a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New extraction timestamp saved: 2025-06-18T12:31:32.820503\n"
     ]
    }
   ],
   "source": [
    "# Get the current date and time to mark when this extrction occurred\n",
    "current_extraction_time = datetime.now()\n",
    "\n",
    "try:\n",
    "    # Attempt to open the timestamp file in write modde W\n",
    "    with open(LAST_EXTRACTION_FILE, 'w') as f:\n",
    "        # Write the current timestamp\n",
    "        f.write(current_extraction_time.isoformat())\n",
    "    \n",
    "    # Confirm successful save with the actual timestamp that was stored\n",
    "    print(f\"New extraction timestamp saved: {current_extraction_time.isoformat()}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    # Handle any errors that might occur during file operations, such as. Permission issues, Disk full, Invalid file path\n",
    "    print(f\"Error saving new timestamp to '{LAST_EXTRACTION_FILE}': {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
